# -*- coding: utf-8 -*-
# /src/scrapers/computrabajo_scraper.py

"""
Scraper espec칤fico para el portal Computrabajo.

Hereda de BaseScraper e implementa la l칩gica particular
para navegar, extraer y normalizar las ofertas de empleo de este sitio.

춰OJO, COMPA칌ERO! Este c칩digo depende mucho de la estructura HTML
de Computrabajo. Si ellos cambian su web, estos selectores CSS
probablemente dejar치n de funcionar y habr치 que actualizarlos.
춰La aventura del scraping! 游땏
"""

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import re # Usaremos expresiones regulares para parsear fechas relativas.
from urllib.parse import quote_plus # Para codificar keywords para la URL.

# Importamos nuestra clase base y el cliente HTTP.
from src.scrapers.base_scraper import BaseScraper
from src.utils.http_client import HTTPClient

# Obtenemos un logger para este scraper espec칤fico.
logger = logging.getLogger(__name__)

# Constante para limitar cu치ntas p치ginas revisar por seguridad (evitar bucles infinitos).
MAX_PAGES_TO_SCRAPE = 10 # Podemos ajustar este n칰mero.

class ComputrabajoScraper(BaseScraper):
    """
    Implementaci칩n del scraper para Computrabajo.

    Se enfoca (inicialmente) en Computrabajo Ecuador (.com.ec).
    """

    def __init__(self, http_client: HTTPClient, config: Optional[Dict[str, Any]] = None):
        """
        Inicializador del scraper de Computrabajo.

        Args:
            http_client (HTTPClient): Instancia del cliente HTTP.
            config (Optional[Dict[str, Any]]): Configuraci칩n espec칤fica para Computrabajo
                                                desde settings.yaml. Esperamos encontrar 'base_url'.
        """
        # Le pasamos el nombre de la fuente y el cliente HTTP a la clase base.
        super().__init__(source_name="computrabajo", http_client=http_client, config=config)

        # Verificamos que tengamos una URL base en la configuraci칩n. 춰Es vital!
        if not self.base_url:
            logger.error(f"[{self.source_name}] No se encontr칩 'base_url' en la configuraci칩n. 춰El scraper no puede funcionar!")
            # Podr칤amos lanzar un error aqu칤 para detener la carga si es cr칤tico.
            # raise ValueError(f"Configuraci칩n para '{self.source_name}' debe incluir 'base_url'.")
            # Por ahora, solo advertimos. fetch_jobs fallar치 si base_url es None.

    def _build_search_url(self, keywords: List[str], location: str, page: int = 1) -> Optional[str]:
        """
        Construye la URL de b칰squeda para Computrabajo Ecuador.

        Args:
            keywords (List[str]): Lista de palabras clave.
            location (str): La ubicaci칩n principal de b칰squeda (ej: "Quito", "Remote Ecuador").
            page (int): El n칰mero de p치gina de resultados a solicitar.

        Returns:
            Optional[str]: La URL construida o None si falta la URL base.
        """
        if not self.base_url:
            logger.error(f"[{self.source_name}] No se puede construir URL de b칰squeda sin 'base_url'.")
            return None

        # Unimos las keywords con '+' o '%20' para la URL. Usaremos quote_plus para seguridad.
        # Ejemplo: ['data', 'analyst'] -> 'data+analyst'
        keyword_query = quote_plus(' '.join(keywords)) if keywords else ''

        # --- L칩gica para la ubicaci칩n (춰Esto puede necesitar ajustes!) ---
        # Computrabajo EC usa provincias y a veces ciudades en par치metros.
        # Necesitamos mapear nuestra 'location' a lo que espera Computrabajo.
        # Asumiremos un mapeo simple por ahora. 춰VERIFICAR ESTO!
        location_query = ''
        if location and location.lower() == 'quito':
             # Para Quito, usualmente se busca por provincia 'Pichincha'. ID 17?
             # Ejemplo (puede variar): &prov=17 o &p=Pichincha
             location_query = "&p=Pichincha" # elif location and 'remote' in location.lower():
            # 쮺칩mo busca remoto Computrabajo? 쯋n filtro? 쯇alabra clave?
            # A침adamos 'remoto' a las keywords si no est치 ya.
            if 'remoto' not in keyword_query.lower() and 'trabajo remoto' not in keyword_query.lower():
                 keyword_query += '+remoto'
            # Quiz치s no haya par치metro de ubicaci칩n espec칤fico para remoto.
            location_query = "" # Dejar vac칤o para buscar en todo el pa칤s (o donde aplique remoto).
        # Podr칤amos a침adir m치s mapeos para otras ubicaciones si expandimos el scope.

        # Construimos la URL final. La estructura puede cambiar.
        # Ejemplo: https://www.computrabajo.com.ec/ofertas-de-trabajo/?q=data+scientist&p=Pichincha&p=1
        # 춰VERIFICAR LA ESTRUCTURA URL REAL!
        search_url = f"{self.base_url}/ofertas-de-trabajo/?q={keyword_query}{location_query}" # # A침adimos el par치metro de paginaci칩n (suele ser 'p' o 'page').
        # 춰VERIFICAR NOMBRE DEL PAR츼METRO DE P츼GINA!
        if page > 1:
            search_url += f"&pg={page}" # logger.debug(f"[{self.source_name}] URL de b칰squeda construida: {search_url}")
        return search_url

    def _parse_relative_date(self, date_str: Optional[str]) -> Optional[str]:
        """
        Intenta convertir fechas relativas ("Hoy", "Ayer", "Hace N d칤as") a formato YYYY-MM-DD.

        Args:
            date_str (Optional[str]): El texto de la fecha extra칤do de la p치gina.

        Returns:
            Optional[str]: La fecha en formato YYYY-MM-DD o el texto original si no se pudo parsear.
        """
        if not date_str:
            return None

        date_str_lower = date_str.lower().strip()
        today = datetime.now().date()

        if 'hoy' in date_str_lower:
            return today.strftime('%Y-%m-%d')
        elif 'ayer' in date_str_lower:
            yesterday = today - timedelta(days=1)
            return yesterday.strftime('%Y-%m-%d')
        elif 'hace' in date_str_lower:
            # Buscamos n칰meros en el string con expresi칩n regular.
            match = re.search(r'hace\s+(\d+)\s+d칤as?', date_str_lower)
            if match:
                try:
                    days_ago = int(match.group(1))
                    past_date = today - timedelta(days=days_ago)
                    return past_date.strftime('%Y-%m-%d')
                except ValueError:
                    logger.warning(f"[{self.source_name}] No se pudo convertir a n칰mero los d칤as en: '{date_str}'")
                    return date_str # Devolver original si falla la conversi칩n
            else:
                # No coincide con "hace N d칤as", puede ser otra cosa.
                return date_str # Devolver original
        else:
            # Si no es relativa conocida, intentamos parsearla como fecha normal
            # o simplemente devolvemos el texto original.
            # Podr칤amos a침adir m치s l칩gica de parseo si Computrabajo usa formatos fijos.
            return date_str # Devolver original por defecto.


    def fetch_jobs(self, search_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Implementaci칩n de la b칰squeda de trabajos para Computrabajo.
        """
        logger.info(f"[{self.source_name}] Iniciando b칰squeda de trabajos con par치metros: {search_params}")
        all_job_offers = []
        current_page = 1

        # Sacamos las keywords y la ubicaci칩n principal de los par치metros.
        # Asumimos que vienen en una lista, las unimos. Puede necesitar ajustes.
        keywords = search_params.get('keywords', [])
        # Por ahora, tomamos la primera ubicaci칩n relevante de la lista.
        # Necesitar칤amos l칩gica m치s robusta si buscamos en varias a la vez.
        location = search_params.get('location', 'Quito') # Default a Quito si no se especifica

        while current_page <= MAX_PAGES_TO_SCRAPE:
            logger.info(f"[{self.source_name}] Procesando p치gina {current_page}...")

            # Construimos la URL para la p치gina actual.
            current_url = self._build_search_url(keywords, location, current_page)
            if not current_url:
                logger.error(f"[{self.source_name}] No se pudo construir la URL para la p치gina {current_page}. Abortando.")
                break # Salir si no podemos ni construir la URL.

            # Descargamos el HTML de la p치gina de listado.
            html_content = self._fetch_html(current_url)
            if not html_content:
                logger.warning(f"[{self.source_name}] No se pudo obtener HTML de la p치gina {current_page}. Terminando paginaci칩n.")
                break # Salir si no hay HTML.

            # Parseamos el HTML.
            soup = self._parse_html(html_content)
            if not soup:
                logger.warning(f"[{self.source_name}] No se pudo parsear HTML de la p치gina {current_page}. Terminando paginaci칩n.")
                break # Salir si no podemos parsear.

            # 춰La parte crucial! Encontrar los contenedores de cada oferta.
            # Necesitamos inspeccionar Computrabajo.com.ec y encontrar el selector CSS correcto.
            # Este selector es SOLO UN EJEMPLO y casi seguro habr치 que cambiarlo.
            job_cards = soup.select('article.box_offer') # if not job_cards:
                logger.info(f"[{self.source_name}] No se encontraron ofertas en la p치gina {current_page}. Parece que es la 칰ltima p치gina.")
                break # No hay m치s ofertas, terminamos.

            logger.info(f"[{self.source_name}] Encontradas {len(job_cards)} ofertas en la p치gina {current_page}.")

            # Procesamos cada tarjeta de oferta encontrada.
            for card in job_cards:
                oferta = self.get_standard_job_dict() # Empezamos con nuestro diccionario est치ndar.

                # --- Extracci칩n de Datos de la Tarjeta ---
                # De nuevo, estos selectores son EJEMPLOS. 춰Hay que verificarlos!

                # T칤tulo y URL de Detalle (suelen estar en el mismo enlace)
                title_link_element = card.select_one('h1.js-o-link a, p.title a') # oferta['titulo'] = self._safe_get_text(title_link_element)
                detail_url_relative = self._safe_get_attribute(title_link_element, 'href')
                # Construimos la URL absoluta si es relativa.
                oferta['url'] = self._build_url(detail_url_relative) if detail_url_relative else None

                # Empresa
                company_element = card.select_one('div.fs16 a, span.d-block a') # oferta['empresa'] = self._safe_get_text(company_element)

                # Ubicaci칩n (puede tener ciudad y provincia)
                location_element = card.select_one('p span:not([class]), span span[title]') # oferta['ubicacion'] = self._safe_get_text(location_element)

                # Fecha de Publicaci칩n (suele ser relativa)
                date_element = card.select_one('span.fc_aux, p.fs13 span') # date_text = self._safe_get_text(date_element)
                oferta['fecha_publicacion'] = self._parse_relative_date(date_text) # Usamos nuestro parser!

                # --- Visitar P치gina de Detalle (Opcional pero recomendado para descripci칩n) ---
                if oferta['url']:
                    logger.debug(f"[{self.source_name}] Visitando p치gina de detalle: {oferta['url']}")
                    detail_html = self._fetch_html(oferta['url'])
                    detail_soup = self._parse_html(detail_html)
                    if detail_soup:
                        # Extraer Descripci칩n
                        # 춰Selector de ejemplo! Hay que buscar el contenedor de la descripci칩n.
                        desc_element = detail_soup.select_one('section#description>div>p, div.fs16') # oferta['descripcion'] = self._safe_get_text(desc_element)
                        # Podr칤amos buscar m치s detalles aqu칤 (salario, tipo contrato, etc.)
                    else:
                        logger.warning(f"[{self.source_name}] No se pudo parsear la p치gina de detalle para: {oferta['url']}")
                else:
                     logger.warning(f"[{self.source_name}] No se encontr칩 URL de detalle para la oferta: {oferta['titulo']}")


                # A침adimos la oferta a nuestra lista si tenemos lo m칤nimo (t칤tulo y URL).
                if oferta['titulo'] and oferta['url']:
                    all_job_offers.append(oferta)
                else:
                    logger.warning(f"[{self.source_name}] Oferta omitida por faltar t칤tulo o URL.")


            # --- L칩gica de Paginaci칩n ---
            # Buscamos el enlace a la p치gina siguiente. 춰Selector de ejemplo!
            next_page_link = soup.select_one('a.arrow_pag > span.icon-right-arrow') # # Otra forma podr칤a ser buscar li.active + li > a

            if next_page_link:
                # Si encontramos el bot칩n/enlace, incrementamos la p치gina.
                # La URL se reconstruir치 al inicio del bucle while.
                current_page += 1
                logger.debug(f"[{self.source_name}] Enlace a p치gina siguiente encontrado. Pasando a p치gina {current_page}.")
            else:
                # Si no hay enlace 'Siguiente', asumimos que es la 칰ltima p치gina.
                logger.info(f"[{self.source_name}] No se encontr칩 enlace a p치gina siguiente en la p치gina {current_page-1}. Terminando paginaci칩n.")
                break # Salimos del bucle while.

        # Fin del bucle while (paginaci칩n)

        logger.info(f"[{self.source_name}] B칰squeda finalizada. Se encontraron {len(all_job_offers)} ofertas en total.")
        return all_job_offers

# --- Ejemplo de uso (si ejecutamos este script directamente) ---
if __name__ == '__main__':
    # Necesitamos configurar logging y http_client para probar.
    from src.utils.logging_config import setup_logging
    from src.utils.http_client import HTTPClient

    # Configuramos logging b치sico para ver los mensajes.
    setup_logging() # Usar치 la config de settings.yaml si existe y es correcta.

    # Creamos el cliente HTTP.
    http_client = HTTPClient()

    # Simulamos la configuraci칩n que vendr칤a de settings.yaml para Computrabajo.
    # 춰Aseg칰rate de poner la URL base correcta para Ecuador!
    computrabajo_config = {
        'enabled': True,
        'base_url': 'https://www.computrabajo.com.ec' # 춰URL para Ecuador!
    }

    # Creamos una instancia de nuestro scraper.
    scraper = ComputrabajoScraper(http_client=http_client, config=computrabajo_config)

    # Definimos par치metros de b칰squeda de ejemplo.
    search_params = {
        'keywords': ['analista', 'datos'],
        'location': 'Quito' # O podr칤a ser 'Remote Ecuador'
    }

    print(f"\n--- Iniciando prueba de ComputrabajoScraper ---")
    print(f"Buscando trabajos con: {search_params}")

    # Llamamos al m칠todo principal.
    try:
        ofertas = scraper.fetch_jobs(search_params)
        print(f"\n--- Prueba finalizada ---")
        print(f"Se encontraron {len(ofertas)} ofertas.")

        # Imprimimos la primera oferta encontrada (si existe) para verla.
        if ofertas:
            print("\nEjemplo de la primera oferta encontrada:")
            # Usamos pprint para que el diccionario se vea m치s bonito.
            import pprint
            pprint.pprint(ofertas[0])
        else:
            print("\nNo se encontraron ofertas con los criterios de prueba.")

    except Exception as e:
        logger.exception("Ocurri칩 un error durante la prueba del scraper.")
        print(f"\n--- Ocurri칩 un error durante la prueba: {e} ---")

    finally:
        # Cerramos el cliente HTTP al final.
        http_client.close()