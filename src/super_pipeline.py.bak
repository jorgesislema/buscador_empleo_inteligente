# -*- coding: utf-8 -*-
# /src/super_pipeline.py

"""
Versi√≥n super mejorada del pipeline de b√∫squeda de empleo, integrando todas las mejoras:
- Soporte para scrapers mejorados (LinkedIn, etc.)
- Cliente HTTP con manejo robusto de errores
- Mejor paralelismo y gesti√≥n de recursos
- Estad√≠sticas detalladas
- Estrategias anti-bloqueo avanzadas
"""

import logging
import sys
import os
import traceback
import time
import json
import random
import concurrent.futures
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional, Union

# Configurar logger
logger = logging.getLogger(__name__)

# Asegurar que podamos importar desde el directorio ra√≠z
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    # Importaciones b√°sicas
    from src.utils import config_loader, logging_config
    from src.persistence.database_manager import DatabaseManager
    from src.persistence import file_exporter
    from src.core.job_filter import JobFilter
    from src.core import data_processor
    from src.apis.base_api import BaseAPIClient
    from src.scrapers.base_scraper import BaseScraper

    # Verificar disponibilidad del cliente HTTP mejorado
    try:
        from src.utils.http_client_improved import ImprovedHTTPClient
        HTTP_CLIENT_IMPROVED_AVAILABLE = True
        HTTPClientClass = ImprovedHTTPClient
    except ImportError:
        from src.utils.http_client import HTTPClient
        HTTP_CLIENT_IMPROVED_AVAILABLE = False
        HTTPClientClass = HTTPClient
        
    # Verificar disponibilidad del manejador de errores mejorado
    try:
        from src.utils.error_handler import register_error, clear_error_registry, get_error_summary, make_search_more_robust
        ERROR_HANDLER_AVAILABLE = True
    except ImportError:
        ERROR_HANDLER_AVAILABLE = False
        # Funciones de fallback si no est√° disponible el error_handler        def register_error(*args, **kwargs): pass
        def clear_error_registry(*args, **kwargs): pass
        def get_error_summary(*args, **kwargs): return {}
        def make_search_more_robust(params): return [params]
    
    # Importaciones de APIs
    from src.apis.arbeitnow_client import ArbeitnowClient
    from src.apis.jobicy_client import JobicyClient
    from src.apis.jooble_client import JoobleClient
    from src.apis.remoteok_client import RemoteOkClient
    from src.apis.huggingface_client import HuggingFaceClient

    # Verificar disponibilidad del scraper de LinkedIn mejorado
    try:
        from src.scrapers.linkedin_scraper_improved import LinkedInScraperImproved as LinkedInScraper
        LINKEDIN_SCRAPER_IMPROVED_AVAILABLE = True
    except ImportError:
        from src.scrapers.linkedin_scraper import LinkedInScraper
        LINKEDIN_SCRAPER_IMPROVED_AVAILABLE = False
        
    # Verificar disponibilidad del scraper de InfoJobs mejorado
    try:
        from src.scrapers.infojobs_scraper_improved import InfojobsScraperImproved as InfojobsScraper
        INFOJOBS_SCRAPER_IMPROVED_AVAILABLE = True
    except ImportError:
        from src.scrapers.infojobs_scraper import InfojobsScraper
        INFOJOBS_SCRAPER_IMPROVED_AVAILABLE = False
        
    # Verificar disponibilidad del scraper de Computrabajo mejorado
    try:
        from src.scrapers.computrabajo_scraper_improved import ComputrabajoScraperImproved as ComputrabajoScraper
        COMPUTRABAJO_SCRAPER_IMPROVED_AVAILABLE = True
    except ImportError:
        from src.scrapers.computrabajo_scraper import ComputrabajoScraper
        COMPUTRABAJO_SCRAPER_IMPROVED_AVAILABLE = False
        
    # Verificar disponibilidad del cliente de Adzuna mejorado
    try:
        from src.apis.adzuna_client_improved import AdzunaClientImproved as AdzunaClient
        ADZUNA_CLIENT_IMPROVED_AVAILABLE = True
    except ImportError:
        from src.apis.adzuna_client import AdzunaClient
        ADZUNA_CLIENT_IMPROVED_AVAILABLE = False# Importaciones de scrapers
    from src.scrapers.bumeran_scraper import BumeranScraper
    from src.scrapers.computrabajo_scraper import ComputrabajoScraper
    from src.scrapers.empleosnet_scraper import EmpleosNetScraper
    from src.scrapers.getonboard_scraper import GetonboardScraper
    from src.scrapers.multitrabajos_scraper import MultitrabajosScraper
    from src.scrapers.opcionempleo_scraper import OpcionempleoScraper
    from src.scrapers.porfinempleo_scraper import PorfinempleoScraper
    from src.scrapers.portalempleoec_scraper import PortalempleoecScraper
    from src.scrapers.remoterocketship_scraper import RemoteRocketshipScraper
    from src.scrapers.soyfreelancer_scraper import SoyFreelancerScraper
    from src.scrapers.tecnoempleo_scraper import TecnoempleoScraper
    from src.scrapers.workana_scraper import WorkanaScraper
    from src.scrapers.wellfound_scraper import WellfoundScraper
except ImportError as e:
    print(f"Error CR√çTICO: No se pudieron importar m√≥dulos esenciales: {e}")
    print("Aseg√∫rate de que la estructura del proyecto y los __init__.py est√©n correctos.")
    print("Verifica tambi√©n que est√°s ejecutando desde la carpeta ra√≠z del proyecto.")
    sys.exit(1)

# Mapeo de fuentes a sus clases
SOURCE_MAP = {
    "adzuna": {"class": AdzunaClient, "type": "apis"},
    "arbeitnow": {"class": ArbeitnowClient, "type": "apis"},
    "jobicy": {"class": JobicyClient, "type": "apis"},
    "jooble": {"class": JoobleClient, "type": "apis"},
    "remoteok": {"class": RemoteOkClient, "type": "apis"},
    "huggingface": {"class": HuggingFaceClient, "type": "apis"},
    "bumeran": {"class": BumeranScraper, "type": "scrapers"},
    "computrabajo": {"class": ComputrabajoScraper, "type": "scrapers"},
    "empleosnet": {"class": EmpleosNetScraper, "type": "scrapers"},
    "getonboard": {"class": GetonboardScraper, "type": "scrapers"},
    "infojobs": {"class": InfojobsScraper, "type": "scrapers"},
    "multitrabajos": {"class": MultitrabajosScraper, "type": "scrapers"},
    "opcionempleo": {"class": OpcionempleoScraper, "type": "scrapers"},
    "porfinempleo": {"class": PorfinempleoScraper, "type": "scrapers"},
    "portalempleoec": {"class": PortalempleoecScraper, "type": "scrapers"},
    "remoterocketship": {"class": RemoteRocketshipScraper, "type": "scrapers"},
    "soyfreelancer": {"class": SoyFreelancerScraper, "type": "scrapers"},
    "tecnoempleo": {"class": TecnoempleoScraper, "type": "scrapers"},
    "workana": {"class": WorkanaScraper, "type": "scrapers"},
    "linkedin": {"class": LinkedInScraper, "type": "scrapers"},
    "wellfound": {"class": WellfoundScraper, "type": "scrapers"},
}

class SuperPipeline:
    """
    Pipeline de b√∫squeda de empleo con todas las mejoras integradas.
    Dise√±ado para maximizar resultados y minimizar errores.
    """
    
    def __init__(self):
        """Inicializa el pipeline con sus componentes principales."""
        self.config = None
        self.http_client = None
        self.db_manager = None
        self.job_filter = None
        self.active_sources = []
        
        # Estad√≠sticas para monitoreo        self.stats = {
            'start_time': datetime.now().isoformat(),
            'end_time': None,
            'duration_seconds': 0,
            'sources': {
                'total': 0,
                'successful': 0,
                'failed': 0,
                'details': {}
            },
            'jobs': {
                'total_raw': 0,
                'total_processed': 0,
                'total_filtered': 0
            },
            'improved_modules': {
                'http_client': HTTP_CLIENT_IMPROVED_AVAILABLE,
                'error_handler': ERROR_HANDLER_AVAILABLE,
                'linkedin_scraper': LINKEDIN_SCRAPER_IMPROVED_AVAILABLE,
                'infojobs_scraper': INFOJOBS_SCRAPER_IMPROVED_AVAILABLE,
                'adzuna_client': ADZUNA_CLIENT_IMPROVED_AVAILABLE
            },
            'error_summary': {}
        }
        
        # Listas para seguimiento
        self.successful_sources = []
        self.failed_sources = []
        
        # Limpiar registro de errores si est√° disponible
        if ERROR_HANDLER_AVAILABLE:
            clear_error_registry()
    
    def initialize(self) -> bool:
        """
        Configura el entorno y carga todos los componentes necesarios.
        
        Returns:
            bool: True si la configuraci√≥n fue exitosa, False en caso contrario
        """
        try:
            # Configurar logging
            logging_config.setup_logging()
            logger.info("*" * 70)
            logger.info("**** Iniciando Super Pipeline de B√∫squeda de Empleo (v2.0) ****")
            logger.info("*" * 70)
            
            # Cargar configuraci√≥n
            self.config = config_loader.get_config()
            if not self.config:
                logger.critical("¬°Fallo al cargar la configuraci√≥n! Abortando pipeline.")
                return False
                
            # Inicializar componentes principales
            logger.info("Inicializando herramientas mejoradas...")
            
            # Usar cliente HTTP mejorado si est√° disponible
            if HTTP_CLIENT_IMPROVED_AVAILABLE:
                logger.info("‚úÖ Usando cliente HTTP mejorado con manejo robusto de errores")
                self.http_client = ImprovedHTTPClient()
            else:
                logger.info("‚ö†Ô∏è Usando cliente HTTP est√°ndar (el mejorado no est√° disponible)")
                self.http_client = HTTPClient()
                
            self.db_manager = DatabaseManager()
            self.job_filter = JobFilter()
              # Informar sobre m√≥dulos mejorados disponibles
            logger.info("Componentes mejorados disponibles:")
            logger.info(f"- Cliente HTTP mejorado: {'‚úÖ Activo' if HTTP_CLIENT_IMPROVED_AVAILABLE else '‚ùå No disponible'}")
            logger.info(f"- Manejador de errores: {'‚úÖ Activo' if ERROR_HANDLER_AVAILABLE else '‚ùå No disponible'}")
            logger.info(f"- LinkedIn Scraper mejorado: {'‚úÖ Activo' if LINKEDIN_SCRAPER_IMPROVED_AVAILABLE else '‚ùå No disponible'}")
            logger.info(f"- InfoJobs Scraper mejorado: {'‚úÖ Activo' if INFOJOBS_SCRAPER_IMPROVED_AVAILABLE else '‚ùå No disponible'}")
            logger.info(f"- Adzuna API Client mejorado: {'‚úÖ Activo' if ADZUNA_CLIENT_IMPROVED_AVAILABLE else '‚ùå No disponible'}")
            
            # Actualizar estad√≠sticas de m√≥dulos disponibles
            self.stats['improved_modules'] = {
                'http_client': HTTP_CLIENT_IMPROVED_AVAILABLE,
                'error_handler': ERROR_HANDLER_AVAILABLE,
                'linkedin_scraper': LINKEDIN_SCRAPER_IMPROVED_AVAILABLE,
                'infojobs_scraper': INFOJOBS_SCRAPER_IMPROVED_AVAILABLE,
                'adzuna_client': ADZUNA_CLIENT_IMPROVED_AVAILABLE
            }
            
            # Cargar fuentes habilitadas
            return self._load_enabled_sources()
            
        except Exception as e:
            logger.critical(f"Error MUY GRAVE en la configuraci√≥n inicial: {e}", exc_info=True)
            return False
    
    def _load_enabled_sources(self) -> bool:
        """
        Carga todas las fuentes habilitadas desde la configuraci√≥n.
        
        Returns:
            bool: True si hay al menos una fuente activa, False si no hay ninguna
        """
        logger.info("Identificando y cargando fuentes habilitadas desde settings.yaml...")
        sources_config = self.config.get('sources', {})
        if not sources_config:
            logger.warning("No se encontr√≥ la secci√≥n 'sources' en la configuraci√≥n.")
            return False
            
        for source_type in ['apis', 'scrapers']:
            for source_name, source_cfg in sources_config.get(source_type, {}).items():
                self.stats['sources']['details'][source_name] = {
                    'type': source_type,
                    'enabled': source_cfg.get('enabled', False),
                    'status': 'pending'
                }
                
                if source_cfg and source_cfg.get('enabled', False):
                    logger.info(f"Fuente '{source_name}' ({source_type}) est√° habilitada.")
                    if source_name in SOURCE_MAP:
                        SourceClass = SOURCE_MAP[source_name]["class"]
                        try:
                            instance = SourceClass(http_client=self.http_client, config=source_cfg)
                            self.active_sources.append(instance)
                            logger.info(f"Instancia de {SourceClass.__name__} creada exitosamente.")
                            
                            # En el caso espec√≠fico de LinkedIn, verificar si estamos usando la versi√≥n mejorada
                            if source_name == 'linkedin' and LINKEDIN_SCRAPER_IMPROVED_AVAILABLE:
                                logger.info(f"‚úÖ Usando versi√≥n mejorada para LinkedIn con caracter√≠sticas avanzadas")
                                
                        except Exception as e:
                            logger.error(f"Error al instanciar {SourceClass.__name__} para '{source_name}'. Se omitir√° esta fuente.", exc_info=True)
                            self.failed_sources.append(f"{source_name} (error de inicializaci√≥n)")
                            self.stats['sources']['details'][source_name]['status'] = 'error_init'
                            self.stats['sources']['details'][source_name]['error'] = str(e)
                            register_error('init_error', source_name, str(e))
                    else:
                        logger.warning(f"Fuente '{source_name}' habilitada en config, ¬°pero no se encontr√≥ su clase en SOURCE_MAP!")
                        self.failed_sources.append(f"{source_name} (no encontrada en SOURCE_MAP)")
                        self.stats['sources']['details'][source_name]['status'] = 'not_found'
        
        self.stats['sources']['total'] = len(self.active_sources)
        logger.info(f"Se han cargado {len(self.active_sources)} fuentes activas")
        
        if not self.active_sources:
            logger.warning("¬°No hay fuentes activas configuradas para ejecutarse! Terminando pipeline.")
            return False
            
        return True

    def _build_search_params(self) -> List[Dict[str, Any]]:
        """
        Construye m√∫ltiples variaciones de par√°metros de b√∫squeda para maximizar
        la cobertura y los resultados.
        
        Returns:
            Lista de diccionarios con par√°metros de b√∫squeda en diferentes combinaciones
        """
        all_keywords = (self.config.get('job_titles', []) or []) + \
                    (self.config.get('tools_technologies', []) or []) + \
                    (self.config.get('topics', []) or [])
        main_location = (self.config.get('locations', []) or [None])[0]

        # Construir varios conjuntos de par√°metros
        search_params_variations = [
            # Par√°metros completos
            {
                'keywords': all_keywords,
                'location': main_location,
                'process_detail_pages': True  # Para scrapers que soporten enriquecimiento
            },
            # Solo palabras clave t√©cnicas/herramientas
            {
                'keywords': self.config.get('tools_technologies', []) or [],
                'location': main_location
            },
            # Solo t√≠tulos de trabajo
            {
                'keywords': self.config.get('job_titles', []) or [],
                'location': main_location
            }
        ]
        
        # Si hay demasiadas keywords, crear versiones con menos keywords
        if len(all_keywords) > 20:
            # Versi√≥n con solo las 10 primeras keywords
            search_params_variations.append({
                'keywords': all_keywords[:10],
                'location': main_location
            })
            # Versi√≥n con keywords 11-20
            search_params_variations.append({
                'keywords': all_keywords[10:20],
                'location': main_location
            })
        
        # Palabras clave t√©cnicas populares para b√∫squeda m√°s espec√≠fica
        tech_keywords = [k for k in all_keywords if k.lower() in 
                       ['python', 'javascript', 'react', 'data', 'developer', 'programador', 
                        'software', 'web', 'frontend', 'backend', 'data scientist']]
        if tech_keywords:
            search_params_variations.append({
                'keywords': tech_keywords[:10],
                'location': main_location
            })
        
        # Versi√≥n sin ubicaci√≥n para fuentes globales
        search_params_variations.append({
            'keywords': all_keywords[:10] if len(all_keywords) > 10 else all_keywords,
            'location': None
        })
        
        # Si tenemos el m√≥dulo de error_handler, usar su funci√≥n para crear m√°s variaciones robustas
        if ERROR_HANDLER_AVAILABLE:
            additional_variations = make_search_more_robust(search_params_variations[0])
            search_params_variations.extend(additional_variations)
        
        # Eliminar duplicados manteniendo el orden
        unique_variations = []
        seen = set()
        for params in search_params_variations:
            # Convertir a tupla inmutable para poder usarlo como clave en un conjunto
            key = (tuple(params.get('keywords', [])), params.get('location'))
            if key not in seen:
                seen.add(key)
                unique_variations.append(params)
        
        logger.info(f"Par√°metros de b√∫squeda globales: {len(all_keywords)} keywords, location hint: '{main_location}'")
        logger.info(f"Se usar√°n {len(unique_variations)} variaciones de par√°metros para maximizar resultados")
        
        return unique_variations

    def _fetch_jobs_from_source(self, source_instance: Union[BaseAPIClient, BaseScraper], 
                              search_params_variations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Obtiene ofertas de trabajo de una fuente espec√≠fica probando diferentes variaciones
        de par√°metros de b√∫squeda.
        
        Args:
            source_instance: Instancia del scraper o cliente API
            search_params_variations: Lista de diferentes par√°metros de b√∫squeda a probar
            
        Returns:
            Lista de ofertas de trabajo encontradas
        """
        source_name = source_instance.source_name
        logger.info(f"üîç Ejecutando fetch_jobs para: {source_name}...")
        
        source_stats = {
            'source_name': source_name,
            'start_time': datetime.now().isoformat(),
            'end_time': None,
            'duration_seconds': 0,
            'jobs_found': 0,
            'variations_tried': 0,
            'status': 'running'
        }
        
        start_time = time.time()
        jobs_from_source = []
        
        try:
            # Para cada variaci√≥n de par√°metros
            for i, params in enumerate(search_params_variations):
                source_stats['variations_tried'] += 1
                
                # Si no es la primera variaci√≥n, a√±adir delay para parecer m√°s humano
                if i > 0:
                    delay = random.uniform(1.5, 3.0)
                    logger.info(f"Esperando {delay:.2f}s antes de probar variaci√≥n {i+1} para {source_name}...")
                    time.sleep(delay)
                
                try:
                    logger.info(f"Usando variaci√≥n {i+1}/{len(search_params_variations)} para {source_name}")
                    
                    # Si esta fuente es LinkedIn y tenemos la versi√≥n mejorada, configurar par√°metros adicionales
                    if source_name == 'linkedin' and LINKEDIN_SCRAPER_IMPROVED_AVAILABLE:
                        # Configurar para procesar p√°ginas de detalle si no est√°n ya configuradas
                        if 'process_detail_pages' not in params:
                            params = params.copy()
                            params['process_detail_pages'] = True
                    
                    # Realizar la b√∫squeda
                    variation_jobs = source_instance.fetch_jobs(params)
                    
                    if variation_jobs:
                        logger.info(f"‚úÖ Variaci√≥n {i+1} encontr√≥ {len(variation_jobs)} ofertas para '{source_name}'")
                        
                        # Verificar posibles duplicados antes de a√±adirlos
                        existing_urls = {job.get('url', '') for job in jobs_from_source if job.get('url')}
                        new_jobs = [job for job in variation_jobs 
                                  if job.get('url') and job.get('url') not in existing_urls]
                        
                        if new_jobs:
                            jobs_from_source.extend(new_jobs)
                            logger.info(f"A√±adidas {len(new_jobs)} nuevas ofertas (no duplicadas) de variaci√≥n {i+1}")
                        else:
                            logger.info(f"Todas las ofertas de variaci√≥n {i+1} eran duplicadas, ninguna a√±adida")
                            
                        # Si ya tenemos suficientes ofertas, paramos
                        if len(jobs_from_source) >= 50:
                            logger.info(f"Alcanzado m√°ximo recomendado de ofertas ({len(jobs_from_source)}) para {source_name}")
                            break
                    else:
                        logger.info(f"‚ùå Variaci√≥n {i+1} no encontr√≥ ofertas para '{source_name}'")
                
                except Exception as e:
                    logger.error(f"Error en variaci√≥n {i+1} para '{source_name}': {str(e)}")
                    register_error('variation_error', source_name, f"Variaci√≥n {i+1}: {str(e)}")
            
            # Procesar resultados finales
            if jobs_from_source:
                # Asegurar que cada oferta tenga el campo 'fuente' correctamente asignado
                for job in jobs_from_source:
                    if 'fuente' not in job or not job['fuente']:
                        job['fuente'] = source_name
                        
                logger.info(f"‚úÖ Fuente '{source_name}' devolvi√≥ {len(jobs_from_source)} ofertas en total.")
                self.successful_sources.append(source_name)
                source_stats['status'] = 'success'
                source_stats['jobs_found'] = len(jobs_from_source)
            else:
                logger.info(f"‚ùå Fuente '{source_name}' no devolvi√≥ ofertas despu√©s de intentar todas las variaciones.")
                self.failed_sources.append(f"{source_name} (sin resultados)")
                source_stats['status'] = 'no_results'
                
        except Exception as e:
            end_time = time.time()
            source_stats['duration_seconds'] = end_time - start_time
            source_stats['end_time'] = datetime.now().isoformat()
            source_stats['status'] = 'error'
            source_stats['error'] = str(e)
            
            logger.error(f"‚õî Error general al ejecutar fetch_jobs para '{source_name}'! Se continuar√° con la siguiente fuente.", exc_info=True)
            self.failed_sources.append(f"{source_name} (error: {str(e)[:100]}...)")
            
            # Registrar error
            register_error('fetch_error', source_name, str(e))
                
            # Actualizar estad√≠sticas
            self.stats['sources']['details'][source_name] = source_stats
            return []
        
        # Actualizar estad√≠sticas finales
        end_time = time.time()
        source_stats['duration_seconds'] = end_time - start_time
        source_stats['end_time'] = datetime.now().isoformat()
        self.stats['sources']['details'][source_name] = source_stats
        
        return jobs_from_source

    def _fetch_jobs_parallel(self, max_workers: int = 3) -> List[Dict[str, Any]]:
        """
        Obtiene ofertas de trabajo de m√∫ltiples fuentes en paralelo con 
        control de concurrencia para evitar sobrecarga.
        
        Args:
            max_workers: N√∫mero m√°ximo de workers en paralelo
            
        Returns:
            Lista combinada de ofertas de trabajo de todas las fuentes
        """
        all_raw_jobs = []
        search_params_variations = self._build_search_params()
        
        # Agrupar las fuentes por tipo (API y scrapers)
        apis = [s for s in self.active_sources if isinstance(s, BaseAPIClient)]
        scrapers = [s for s in self.active_sources if isinstance(s, BaseScraper)]
        
        logger.info(f"Ejecutando {len(apis)} APIs y {len(scrapers)} scrapers")
        
        # Primero ejecutamos las APIs en paralelo (son m√°s r√°pidas y estables)
        if apis:
            logger.info(f"Ejecutando {len(apis)} APIs en paralelo (max_workers={max_workers})...")
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Crear las tareas para cada API
                future_to_api = {
                    executor.submit(self._fetch_jobs_from_source, api, search_params_variations): api 
                    for api in apis
                }
                
                # Procesar los resultados a medida que est√©n disponibles
                for future in concurrent.futures.as_completed(future_to_api):
                    api = future_to_api[future]
                    try:
                        jobs = future.result()
                        if jobs:
                            all_raw_jobs.extend(jobs)
                            logger.info(f"API '{api.source_name}' completada con {len(jobs)} ofertas")
                    except Exception as e:
                        logger.error(f"Error procesando resultados de API '{api.source_name}': {e}")
        
        # Luego ejecutamos los scrapers con menos paralelismo para evitar bloqueos
        scraper_workers = min(2, max_workers)  # Limitamos a 2 scrapers en paralelo
        if scrapers:
            logger.info(f"Ejecutando {len(scrapers)} scrapers (max_workers={scraper_workers})...")
            with concurrent.futures.ThreadPoolExecutor(max_workers=scraper_workers) as executor:
                # Crear las tareas para cada scraper
                future_to_scraper = {
                    executor.submit(self._fetch_jobs_from_source, scraper, search_params_variations): scraper 
                    for scraper in scrapers
                }
                
                # Procesar los resultados a medida que est√©n disponibles
                for future in concurrent.futures.as_completed(future_to_scraper):
                    scraper = future_to_scraper[future]
                    try:
                        jobs = future.result()
                        if jobs:
                            all_raw_jobs.extend(jobs)
                            logger.info(f"Scraper '{scraper.source_name}' completado con {len(jobs)} ofertas")
                    except Exception as e:
                        logger.error(f"Error procesando resultados de scraper '{scraper.source_name}': {e}")
        
        # Actualizar estad√≠sticas
        self.stats['sources']['successful'] = len(self.successful_sources)
        self.stats['sources']['failed'] = len(self.failed_sources)
        self.stats['jobs']['total_raw'] = len(all_raw_jobs)
        
        return all_raw_jobs
        
    def run(self) -> Dict[str, Any]:
        """
        Ejecuta toda la pipeline de b√∫squeda de empleo.
        
        Returns:
            Dict: Resultados y estad√≠sticas de la ejecuci√≥n
        """
        if not self.initialize():
            return {
                "status": "error",
                "message": "Error en la inicializaci√≥n de la pipeline",
                "stats": self.stats
            }
            
        start_time = time.time()
        
        try:
            # Realizar la recolecci√≥n de datos (con paralelismo controlado)
            logger.info(f"--- Iniciando Recolecci√≥n de Datos ({len(self.active_sources)} fuentes activas) ---")
            all_raw_jobs = self._fetch_jobs_parallel()
            
            # Si no se encontraron ofertas, terminar
            if not all_raw_jobs:
                logger.warning("‚ö†Ô∏è No se encontraron ofertas de empleo en ninguna fuente.")
                self.cleanup()
                self.stats['end_time'] = datetime.now().isoformat()
                self.stats['duration_seconds'] = time.time() - start_time
                return {
                    "status": "warning",
                    "message": "No se encontraron ofertas de empleo",
                    "stats": self.stats
                }
            
            # Resumen de fuentes exitosas y fallidas
            logger.info(f"--- Recolecci√≥n Finalizada. Total ofertas 'crudas' obtenidas: {len(all_raw_jobs)} ---")
            logger.info(f"Fuentes exitosas ({len(self.successful_sources)}): {', '.join(self.successful_sources)}")
            logger.info(f"Fuentes sin resultados o con errores ({len(self.failed_sources)}): {', '.join(self.failed_sources)}")

            # Procesamiento y limpieza de datos
            logger.info("--- Iniciando Procesamiento/Limpieza de Datos ---")
            processed_jobs = data_processor.process_job_offers(all_raw_jobs)
            logger.info(f"--- Procesamiento Finalizado. {len(processed_jobs)} ofertas despu√©s de limpieza ---")
            self.stats['jobs']['total_processed'] = len(processed_jobs)

            # Filtrado de ofertas
            logger.info("--- Iniciando Filtrado de Ofertas ---")
            filtered_jobs = self.job_filter.filter_jobs(processed_jobs)
            logger.info(f"--- Filtrado Finalizado. {len(filtered_jobs)} ofertas cumplen los criterios ---")
            self.stats['jobs']['total_filtered'] = len(filtered_jobs)

            # Inserci√≥n en base de datos
            if filtered_jobs:
                logger.info("--- Iniciando Inserci√≥n en Base de Datos ---")
                self.db_manager.insert_job_offers(filtered_jobs)
                logger.info("--- Inserci√≥n en Base de Datos Finalizada ---")
            else:
                logger.info("No hay ofertas filtradas para insertar en la base de datos.")

            # Exportaci√≥n a CSV si est√° habilitada
            csv_export_enabled = self.config.get('data_storage', {}).get('csv', {}).get('export_enabled', False)
            if csv_export_enabled:
                logger.info("--- Iniciando Exportaci√≥n a CSV ---")
                if filtered_jobs:
                    # Exportar ambos archivos: todas las ofertas sin filtrar y solo las filtradas
                    file_exporter.export_to_csv(filtered_jobs, is_filtered=True, unfiltered_offers=processed_jobs)
                    logger.info("--- Exportaci√≥n a CSV Finalizada (ofertas filtradas y sin filtrar) ---")
                else:
                    # Si no hay ofertas filtradas, exportar solo las sin filtrar
                    logger.info("No hay ofertas filtradas para exportar, pero se exportar√°n todas las ofertas sin filtrar")
                    file_exporter.export_to_csv([], is_filtered=True, unfiltered_offers=processed_jobs)
                    logger.info("--- Exportaci√≥n a CSV Finalizada (solo ofertas sin filtrar) ---")
            else:
                logger.info("Exportaci√≥n a CSV deshabilitada en la configuraci√≥n.")
            
            # Finalizar y guardar estad√≠sticas
            end_time = time.time()
            self.stats['end_time'] = datetime.now().isoformat()
            self.stats['duration_seconds'] = end_time - start_time
            
            # Guardar estad√≠sticas detalladas en archivo JSON
            self._save_stats()
            
            # Limpieza final
            self.cleanup()
            
            logger.info("******************************************************")
            logger.info("**** Super Pipeline de B√∫squeda de Empleo Finalizado ****")
            logger.info(f"**** Tiempo total: {self.stats['duration_seconds']:.2f} segundos ****")
            logger.info(f"**** Ofertas encontradas: {self.stats['jobs']['total_filtered']} / {self.stats['jobs']['total_raw']} ****")
            logger.info("******************************************************")
            
            return {
                "status": "success",
                "message": "Pipeline ejecutado exitosamente",
                "stats": self.stats,
                "processed_jobs": len(processed_jobs),
                "filtered_jobs": len(filtered_jobs)
            }
            
        except Exception as e:
            logger.critical(f"Error grave durante la ejecuci√≥n de la pipeline: {e}", exc_info=True)
            self.cleanup()
            self.stats['end_time'] = datetime.now().isoformat()
            self.stats['duration_seconds'] = time.time() - start_time
            self.stats['error'] = str(e)
            return {
                "status": "error",
                "message": f"Error grave durante la ejecuci√≥n: {str(e)}",
                "stats": self.stats
            }
    
    def _save_stats(self):
        """Guarda las estad√≠sticas detalladas en un archivo JSON."""
        try:
            stats_dir = project_root / "data" / "stats"
            stats_dir.mkdir(exist_ok=True, parents=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            stats_file = stats_dir / f"pipeline_stats_{timestamp}.json"
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
                
            logger.info(f"Estad√≠sticas guardadas en {stats_file}")
        except Exception as e:
            logger.error(f"Error guardando estad√≠sticas: {e}")
    
    def cleanup(self):
        """Realiza limpieza de recursos al finalizar."""
        if self.http_client:
            logger.info("Cerrando cliente HTTP...")
            self.http_client.close()
            
            # Si estamos usando el cliente mejorado, mostrar estad√≠sticas
            if HTTP_CLIENT_IMPROVED_AVAILABLE and hasattr(self.http_client, 'get_stats'):
                http_stats = self.http_client.get_stats()
                logger.info(f"Estad√≠sticas de HTTP: {http_stats.get('success_rate', '?')} de √©xito en {http_stats.get('total_requests', 0)} peticiones")
                
                problematic_domains = self.http_client.get_problematic_domains() if hasattr(self.http_client, 'get_problematic_domains') else {}
                if problematic_domains:
                    logger.warning(f"Dominios problem√°ticos detectados: {', '.join(problematic_domains.keys())}")


def run_job_search_pipeline_super():
    """
    Funci√≥n principal que inicia el super pipeline de b√∫squeda de empleo.
    """
    pipeline = SuperPipeline()
    result = pipeline.run()
    return result


if __name__ == "__main__":
    print("Ejecutando el super pipeline de b√∫squeda de empleo...")
    try:
        result = run_job_search_pipeline_super()
        
        if result["status"] == "success":
            print("\n‚úÖ Super pipeline ejecutado exitosamente.")
            print(f"Se encontraron {result.get('filtered_jobs', 0)} ofertas filtradas.")
        else:
            print(f"\n‚ö†Ô∏è {result['status'].upper()}: {result['message']}")
            print("Revisa los logs ('logs/app.log') para m√°s detalles.")
    except Exception as e:
        logging.getLogger().critical("¬°Ocurri√≥ un error fatal en la ejecuci√≥n principal!", exc_info=True)
        print(f"\n‚õî ¬°ERROR! El pipeline fall√≥. Revisa los logs ('logs/app.log'). Error: {e}")
